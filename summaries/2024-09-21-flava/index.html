<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> FLAVA: A Foundation Language And Vision Alignment Model | Aarti Sharma </title> <meta name="author" content="Aarti Sharma"> <meta name="description" content="Collections of my thoughts, work and notes. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://4arti.github.io/summaries/2024-09-21-flava/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Aarti</span> Sharma </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/summaries/">ML Paper Summaries <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div style="display:none"> $$ \newcommand{\bone}{\mathbf{1}} \newcommand{\bbeta}{\mathbf{\beta}} \newcommand{\bdelta}{\mathbf{\delta}} \newcommand{\bepsilon}{\mathbf{\epsilon}} \newcommand{\blambda}{\mathbf{\lambda}} \newcommand{\bomega}{\mathbf{\omega}} \newcommand{\bpi}{\mathbf{\pi}} \newcommand{\bphi}{\mathbf{\phi}} \newcommand{\bvphi}{\mathbf{\varphi}} \newcommand{\bpsi}{\mathbf{\psi}} \newcommand{\bsigma}{\mathbf{\sigma}} \newcommand{\btheta}{\mathbf{\theta}} \newcommand{\btau}{\mathbf{\tau}} \newcommand{\ba}{\mathbf{a}} \newcommand{\bb}{\mathbf{b}} \newcommand{\bc}{\mathbf{c}} \newcommand{\bd}{\mathbf{d}} \newcommand{\be}{\mathbf{e}} \newcommand{\boldf}{\mathbf{f}} \newcommand{\bg}{\mathbf{g}} \newcommand{\bh}{\mathbf{h}} \newcommand{\bi}{\mathbf{i}} \newcommand{\bj}{\mathbf{j}} \newcommand{\bk}{\mathbf{k}} \newcommand{\bell}{\mathbf{\ell}} \newcommand{\bm}{\mathbf{m}} \newcommand{\bn}{\mathbf{n}} \newcommand{\bo}{\mathbf{o}} \newcommand{\bp}{\mathbf{p}} \newcommand{\bq}{\mathbf{q}} \newcommand{\br}{\mathbf{r}} \newcommand{\bs}{\mathbf{s}} \newcommand{\bt}{\mathbf{t}} \newcommand{\bu}{\mathbf{u}} \newcommand{\bv}{\mathbf{v}} \newcommand{\bw}{\mathbf{w}} \newcommand{\bx}{\mathbf{x}} \newcommand{\by}{\mathbf{y}} \newcommand{\bz}{\mathbf{z}} \newcommand{\bA}{\mathbf{A}} \newcommand{\bB}{\mathbf{B}} \newcommand{\bC}{\mathbf{C}} \newcommand{\bD}{\mathbf{D}} \newcommand{\bE}{\mathbf{E}} \newcommand{\bF}{\mathbf{F}} \newcommand{\bG}{\mathbf{G}} \newcommand{\bH}{\mathbf{H}} \newcommand{\bI}{\mathbf{I}} \newcommand{\bJ}{\mathbf{J}} \newcommand{\bK}{\mathbf{K}} \newcommand{\bL}{\mathbf{L}} \newcommand{\bM}{\mathbf{M}} \newcommand{\bN}{\mathbf{N}} \newcommand{\bP}{\mathbf{P}} \newcommand{\bQ}{\mathbf{Q}} \newcommand{\bR}{\mathbf{R}} \newcommand{\bS}{\mathbf{S}} \newcommand{\bT}{\mathbf{T}} \newcommand{\bU}{\mathbf{U}} \newcommand{\bV}{\mathbf{V}} \newcommand{\bW}{\mathbf{W}} \newcommand{\bX}{\mathbf{X}} \newcommand{\bY}{\mathbf{Y}} \newcommand{\bZ}{\mathbf{Z}} \newcommand{\calA}{\mathcal{A}} \newcommand{\calB}{\mathcal{B}} \newcommand{\calC}{\mathcal{C}} \newcommand{\calD}{\mathcal{D}} \newcommand{\calE}{\mathcal{E}} \newcommand{\calF}{\mathcal{F}} \newcommand{\calG}{\mathcal{G}} \newcommand{\calH}{\mathcal{H}} \newcommand{\calI}{\mathcal{I}} \newcommand{\calJ}{\mathcal{J}} \newcommand{\calK}{\mathcal{K}} \newcommand{\calL}{\mathcal{L}} \newcommand{\calM}{\mathcal{M}} \newcommand{\calN}{\mathcal{N}} \newcommand{\calO}{\mathcal{O}} \newcommand{\calP}{\mathcal{P}} \newcommand{\calQ}{\mathcal{Q}} \newcommand{\calR}{\mathcal{R}} \newcommand{\calS}{\mathcal{S}} \newcommand{\calT}{\mathcal{T}} \newcommand{\calU}{\mathcal{U}} \newcommand{\calV}{\mathcal{V}} \newcommand{\calW}{\mathcal{W}} \newcommand{\calX}{\mathcal{X}} \newcommand{\calY}{\mathcal{Y}} \newcommand{\calZ}{\mathcal{Z}} \newcommand{\R}{\mathbb{R}} \newcommand{\C}{\mathbb{C}} \newcommand{\N}{\mathbb{N}} \newcommand{\Z}{\mathbb{Z}} \newcommand{\F}{\mathbb{F}} \newcommand{\Q}{\mathbb{Q}} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} \newcommand{\nnz}[1]{\mbox{nnz}(#1)} \newcommand{\dotprod}[2]{\langle #1, #2 \rangle} \newcommand{\ignore}[1]{} \let\Pr\relax \DeclareMathOperator*{\Pr}{\mathbf{Pr}} \newcommand{\E}{\mathbb{E}} \DeclareMathOperator*{\Ex}{\mathbf{E}} \DeclareMathOperator*{\Var}{\mathbf{Var}} \DeclareMathOperator*{\Cov}{\mathbf{Cov}} \DeclareMathOperator*{\stddev}{\mathbf{stddev}} \DeclareMathOperator*{\avg}{avg} \DeclareMathOperator{\poly}{poly} \DeclareMathOperator{\polylog}{polylog} \DeclareMathOperator{\size}{size} \DeclareMathOperator{\sgn}{sgn} \DeclareMathOperator{\dist}{dist} \DeclareMathOperator{\vol}{vol} \DeclareMathOperator{\spn}{span} \DeclareMathOperator{\supp}{supp} \DeclareMathOperator{\tr}{tr} \DeclareMathOperator{\Tr}{Tr} \DeclareMathOperator{\codim}{codim} \DeclareMathOperator{\diag}{diag} \newcommand{\PTIME}{\mathsf{P}} \newcommand{\LOGSPACE}{\mathsf{L}} \newcommand{\ZPP}{\mathsf{ZPP}} \newcommand{\RP}{\mathsf{RP}} \newcommand{\BPP}{\mathsf{BPP}} \newcommand{\P}{\mathsf{P}} \newcommand{\NP}{\mathsf{NP}} \newcommand{\TC}{\mathsf{TC}} \newcommand{\AC}{\mathsf{AC}} \newcommand{\SC}{\mathsf{SC}} \newcommand{\SZK}{\mathsf{SZK}} \newcommand{\AM}{\mathsf{AM}} \newcommand{\IP}{\mathsf{IP}} \newcommand{\PSPACE}{\mathsf{PSPACE}} \newcommand{\EXP}{\mathsf{EXP}} \newcommand{\MIP}{\mathsf{MIP}} \newcommand{\NEXP}{\mathsf{NEXP}} \newcommand{\BQP}{\mathsf{BQP}} \newcommand{\distP}{\mathsf{dist\textbf{P}}} \newcommand{\distNP}{\mathsf{dist\textbf{NP}}} \newcommand{\eps}{\epsilon} \newcommand{\lam}{\lambda} \newcommand{\dleta}{\delta} \newcommand{\simga}{\sigma} \newcommand{\vphi}{\varphi} \newcommand{\la}{\langle} \newcommand{\ra}{\rangle} \newcommand{\wt}[1]{\widetilde{#1}} \newcommand{\wh}[1]{\widehat{#1}} \newcommand{\ol}[1]{\overline{#1}} \newcommand{\ul}[1]{\underline{#1}} \newcommand{\ot}{\otimes} \newcommand{\zo}{\{0,1\}} \newcommand{\co}{:} %\newcommand{\co}{\colon} \newcommand{\bdry}{\partial} \newcommand{\grad}{\nabla} \newcommand{\transp}{^\intercal} \newcommand{\inv}{^{-1}} \newcommand{\symmdiff}{\triangle} \newcommand{\symdiff}{\symmdiff} \newcommand{\half}{\tfrac{1}{2}} \newcommand{\bbone}{\mathbbm 1} \newcommand{\Id}{\bbone} \newcommand{\SAT}{\mathsf{SAT}} \newcommand{\bcalG}{\boldsymbol{\calG}} \newcommand{\calbG}{\bcalG} \newcommand{\bcalX}{\boldsymbol{\calX}} \newcommand{\calbX}{\bcalX} \newcommand{\bcalY}{\boldsymbol{\calY}} \newcommand{\calbY}{\bcalY} \newcommand{\bcalZ}{\boldsymbol{\calZ}} \newcommand{\calbZ}{\bcalZ} $$ </div> <div class="publications"> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row no-count"> <div id="singh2022flavafoundationallanguagevision" class="col-sm-12"> <div class="title"><h2> <a href="https://arxiv.org/abs/2112.04482" rel="external nofollow noopener" target="_blank"> FLAVA: A Foundational Language And Vision Alignment Model </a> </h2></div> <div class="author"> Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> Jun 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2112.04482" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">singh2022flavafoundationallanguagevision</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FLAVA: A Foundational Language And Vision Alignment Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2112.04482}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CV}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2112.04482}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <div class="post"> <article class="post-content"> <div id="markdown-content"> <blockquote> <p><strong>Objective</strong> :</p> <ul> <li>Understanding the key elements of FLAVA</li> <li>Highlighting its limitations</li> <li>Extracting valuable insights for the future of multimodal AI</li> </ul> </blockquote> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flava.PNG-480.webp 480w,/assets/img/flava.PNG-800.webp 800w,/assets/img/flava.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/flava.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="FLAVA Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">FLAVA Architecture</figcaption> </figure> </div> </div> <hr> <h2 id="1-three-key-innovations-of-flava">1. Three Key Innovations of FLAVA</h2> <h4 id="11-unified-multimodal-pretraining">1.1 Unified Multimodal Pretraining</h4> <p>FLAVA sets out to create a “universal” model that can effectively handle vision tasks, language tasks, and multimodal reasoning. This contrasts with many existing models like CLIP and ALIGN that perform well in specific domains but struggle when required to handle tasks that require deep integration between vision and language.</p> <p><strong>Why This Matters:</strong><br> The ability to fuse vision and language seamlessly could lead to breakthroughs in real-world applications such as autonomous systems, healthcare, and robotics, where multimodal understanding is key to solving complex tasks. FLAVA’s ability to pretrain on both unimodal and multimodal data represents a huge leap towards developing a generalizable AI.</p> <h4 id="12-multimodal-pretraining-objectives">1.2 Multimodal Pretraining Objectives</h4> <p>FLAVA introduces a variety of innovative training objectives:</p> <ul> <li> <strong>Global Contrastive Loss</strong>: Inspired by CLIP, this loss ensures that the model learns meaningful relationships between images and texts.</li> <li> <strong>Masked Multimodal Modeling (MMM)</strong>: This pretraining strategy is a novel extension of masked language models like BERT, but it is applied to both images and texts, significantly improving multimodal task performance.</li> <li> <strong>Image-Text Matching (ITM)</strong>: This objective helps the model discern whether an image and text are correctly paired, further reinforcing multimodal learning.</li> </ul> <p><strong>Why This Matters:</strong><br> These pretraining objectives are designed to make the model adaptable across vision-only, language-only, and vision-and-language tasks. For example, in visual question answering (VQA), the combination of these objectives makes FLAVA highly effective in learning how visual and textual information interrelates.</p> <h4 id="13-open-dataset-approach">1.3 Open Dataset Approach</h4> <p>One of FLAVA’s most attractive features is its reliance on publicly available datasets, totaling 70M image-text pairs. Compared to models like ALIGN (which used 1.8B pairs from private datasets), FLAVA democratizes access to research resources by training on publicly accessible data. This enables more researchers to reproduce and build on this work, fostering a more open scientific community.</p> <p><strong>Why This Matters:</strong><br> The transparency of FLAVA’s dataset usage not only accelerates reproducibility but also ensures a broader, more ethical application of AI technologies, as researchers can avoid the opaque, closed datasets used by models like ALIGN and CLIP.</p> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flavaversusclip.PNG-480.webp 480w,/assets/img/flavaversusclip.PNG-800.webp 800w,/assets/img/flavaversusclip.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/flavaversusclip.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="FLAVA performance comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Performance Difference Between FLAVA and CLIP-ViT-B/16 on Vision, Language, and Multimodal Tasks</figcaption> </figure> </div> </div> <hr> <h2 id="2-one-glaring-deficiency-handling-massive-data">2. One Glaring Deficiency: Handling Massive Data</h2> <h4 id="21-struggling-with-large-scale-tasks">2.1 Struggling with Large-Scale Tasks</h4> <p>Despite its impressive performance across various modalities, FLAVA struggles when scaling to datasets with orders of magnitude more data, such as those handled by CLIP or ALIGN. FLAVA’s training on “only” 70M image-text pairs, while groundbreaking in its transparency, puts it at a disadvantage when compared to models that utilize massive private datasets.</p> <p><strong>The Deficiency:</strong><br> FLAVA’s general performance still lags slightly behind other state-of-the-art models when scaling vision-specific or large-scale multimodal tasks, particularly in domains where the volume of training data is critical.</p> <p><strong>Why This Matters:</strong><br> This limitation could make FLAVA less appealing in commercial applications where large-scale data dominates, such as autonomous vehicle systems or large content moderation systems in social media platforms.</p> <h4 id="22-optimization-challenges">2.2 Optimization Challenges</h4> <p>Moreover, the paper highlights how adding different tasks—vision, language, and multimodal reasoning—together creates optimization complexity. The model performs slightly better on individual vision or language tasks, but struggles to maintain that performance across the board when attempting to merge the training on all modalities.</p> <p><strong>The Deficiency:</strong><br> This indicates that while FLAVA has made strides in multimodal integration, it still faces hurdles when it comes to optimizing and balancing tasks across domains simultaneously. Improving the model’s ability to handle this balance would make it more versatile and robust.</p> <hr> <h2 id="3-conclusion-flavas-role-in-the-future-of-multimodal-ai">3. Conclusion: FLAVA’s Role in the Future of Multimodal AI</h2> <p>FLAVA marks an impressive step toward a universal model capable of handling vision, language, and their fusion. Its innovative pretraining objectives, such as MMM and global contrastive loss, enable it to excel in multimodal tasks, despite using a much smaller dataset than competitors like ALIGN and CLIP.</p> <p><strong>My Insight:</strong><br> The key challenge for FLAVA, and indeed for any foundational model, will be scaling while maintaining high performance across all domains. While FLAVA’s reliance on public datasets is a powerful move toward open science, expanding its training to include much larger datasets will likely be necessary to fully compete with closed-source models. Nonetheless, FLAVA’s transparency and accessibility make it a promising platform for researchers aiming to push the boundaries of multimodal AI.</p> <hr> <h2 id="references">References</h2> <ul> <li>FLAVA Paper: <a href="https://flava-model.github.io/" rel="external nofollow noopener" target="_blank">FLAVA: A Foundational Language and Vision Alignment Model</a> </li> <li>CLIP Paper: <a href="https://openai.com/research/clip" rel="external nofollow noopener" target="_blank">CLIP: Contrastive Language–Image Pretraining</a> </li> <li>ALIGN Paper: <a href="https://arxiv.org/pdf/2104.06283.pdf" rel="external nofollow noopener" target="_blank">Scaling up Visual and Vision-Language Representation Learning</a> </li> </ul> <hr> </div> </article> <p class="post-meta">Written 2024</p> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Aarti Sharma. </div> </footer> <script src="" integrity="" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="" integrity="" crossorigin="anonymous"></script> <script defer src="" integrity="" crossorigin="anonymous"></script> <script defer src="" integrity="" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="" integrity="" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="" integrity="" crossorigin="anonymous"></script> <script defer src="" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script> </script> </body> </html>