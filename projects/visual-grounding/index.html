<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Visual Grounding with Multimodal Attention and Grad-CAM in Image-Text Alignment | Aarti Sharma </title> <meta name="author" content="Aarti Sharma"> <meta name="description" content="FLAVA's image-text alignment by observing attention maps and Grad-CAM outputs in visual grounding"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sharma-aarti.github.io/projects/visual-grounding/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Aarti</span> Sharma </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/summaries/">ML Paper Summaries </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Visual Grounding with Multimodal Attention and Grad-CAM in Image-Text Alignment</h1> <p class="post-description">FLAVA's image-text alignment by observing attention maps and Grad-CAM outputs in visual grounding</p> </header> <article> <p>Visual grounding is a task in computer vision that involves identifying and localizing objects in an image based on a given natural language description. The goal is to “ground” or match a specific object or region in the image that corresponds to the description or referring expression.</p> <p>For example, in the task of visual grounding, a model might be given an image of a park with several people and objects, along with a textual input like “the person wearing a red jacket.” The model must then correctly identify and highlight the person in the red jacket from the image.</p> <p>This task is particularly challenging because it requires understanding both the semantics of the language and the visual information in the image. The model must integrate these two sources of information to precisely locate the object mentioned in the text. Visual grounding is important for various applications, including:</p> <p><strong>Human-robot interaction</strong>: Robots can follow natural language instructions by locating objects in their surroundings.</p> <p><strong>Autonomous driving</strong>: Identifying objects based on spoken commands or navigation instructions.</p> <p><strong>Image captioning and generation</strong>: Improving the alignment between image content and descriptive text.</p> <p>We will delve into how multimodal transformers like FLAVA can be employed to visualize the alignment between images and corresponding textual descriptions. We will do this using two key techniques:</p> <ol> <li> <p><strong>Attention Maps</strong>: Capturing how the model focuses on different regions of the image for specific text tokens.</p> </li> <li> <p><strong>Grad-CAM</strong>: A powerful technique from CNNs that helps us visualize which parts of the image influence the model’s predictions the most.</p> </li> </ol> <p>We will process random images from the Flickr30k dataset, visualize attention maps, and overlay Grad-CAM heatmaps. This will allow us to better understand the interaction between the image and text in the context of visual grounding.</p> <h2 id="importing-required-libraries"><strong>Importing Required Libraries</strong></h2> <p>Before we begin, we will need to import all the necessary libraries. This includes:</p> <ul> <li> <strong>Pytorch</strong>: For handling tensors and backpropagation.</li> <li> <strong>Transformers</strong>: To use FLAVA, the multimodal model.</li> <li> <strong>Matplotlib</strong> and <strong>PIL</strong>: For image visualization and manipulation.</li> <li> <strong>OpenCV</strong>: For overlaying heatmaps on images using Grad-CAM.</li> </ul> <p>Here’s why each library is important:</p> <ul> <li> <strong>Transformers</strong> provides the FLAVA model and processor, essential for multimodal tasks.</li> <li> <strong>Torch</strong> provides the core infrastructure to handle deep learning operations.</li> <li> <strong>Matplotlib</strong> and <strong>PIL</strong> help in rendering the image and heatmap outputs for better interpretability.</li> <li> <strong>OpenCV</strong> is a robust library for computer vision tasks and is used here to blend heatmaps and images effectively.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">csv</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">FlavaProcessor</span><span class="p">,</span> <span class="n">FlavaModel</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">cv2</span>
</code></pre></div></div> <h2 id="downloading-flickr30k-dataset"><strong>Downloading Flickr30k Dataset</strong></h2> <p>The Flickr30k dataset is a popular benchmark used in tasks related to image-captioning, visual grounding, and multimodal learning. It contains the following key features:</p> <p><strong>Dataset Composition</strong>: Flickr30k includes 31,783 images, each paired with five human-generated textual descriptions. These images are primarily sourced from Flickr and feature a wide range of real-world scenarios, including people, animals, and everyday objects in various settings.</p> <p><strong>Multiple Annotations</strong>: Each image in the dataset is paired with multiple captions, making it valuable for exploring the relationship between visual content and language. This richness allows models to learn diverse and contextually nuanced associations between objects and the words used to describe them.</p> <p><strong>Diversity of Visual Content</strong>: The dataset is diverse in terms of objects, people, and environments, providing a challenging benchmark for models aiming to perform well on complex image-text tasks, such as image captioning, visual question answering, and visual grounding.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">wget</span> <span class="sh">"</span><span class="s">https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr30k_part00</span><span class="sh">"</span>
<span class="err">!</span><span class="n">wget</span> <span class="sh">"</span><span class="s">https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr30k_part01</span><span class="sh">"</span>
<span class="err">!</span><span class="n">wget</span> <span class="sh">"</span><span class="s">https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr30k_part02</span><span class="sh">"</span>
<span class="err">!</span><span class="n">cat</span> <span class="n">flickr30k_part00</span> <span class="n">flickr30k_part01</span> <span class="n">flickr30k_part02</span> <span class="o">&gt;</span> <span class="n">flickr30k</span><span class="p">.</span><span class="nb">zip</span>
<span class="err">!</span><span class="n">rm</span> <span class="n">flickr30k_part00</span> <span class="n">flickr30k_part01</span> <span class="n">flickr30k_part02</span>
<span class="err">!</span><span class="n">unzip</span> <span class="o">-</span><span class="n">q</span> <span class="n">flickr30k</span><span class="p">.</span><span class="nb">zip</span> <span class="o">-</span><span class="n">d</span> <span class="p">.</span><span class="o">/</span><span class="n">flickr30k</span>
<span class="err">!</span><span class="n">rm</span> <span class="n">flickr30k</span><span class="p">.</span><span class="nb">zip</span>
<span class="err">!</span><span class="n">echo</span> <span class="sh">"</span><span class="s">Downloaded Flickr30k dataset successfully.</span><span class="sh">"</span>

</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> Downloaded Flickr30k dataset successfully.
</code></pre></div></div> <h2 id="hook-for-capturing-attention-weights"><strong>Hook for Capturing Attention Weights</strong></h2> <p>we define a hook to extract attention weights from the FLAVA model’s image encoder. Attention mechanisms are vital in multimodal models as they allow us to understand how the model is focusing on different parts of the input (in our case, the image) while processing the text.</p> <p>This step is crucial for:</p> <p><strong>Extracting Attention Weights</strong>: Helps us later visualize where the model is looking when aligning text with the image.</p> <p><strong>Registering Hooks</strong>: A feature in PyTorch that lets us capture intermediate outputs during model inference.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define global variable to store attention weights
</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Hook function to capture and print the output from attention layers
</span><span class="k">def</span> <span class="nf">get_attention_weights_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">attention_weights</span>
    <span class="n">attention_weights</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div> <h2 id="preprocessing-images-and-captions"><strong>Preprocessing Images and Captions</strong></h2> <p>Since FLAVA expects both images and text inputs, we need to prepare these inputs accordingly:</p> <p><strong>Image Preprocessing</strong>: We resize the images to match the input size expected by the model (224x224 pixels).</p> <p><strong>Text Processing</strong>: We use the FLAVA processor to tokenize the caption text and prepare it for model input.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Function to preprocess image and caption
</span><span class="k">def</span> <span class="nf">preprocess_image_caption</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">caption</span><span class="p">,</span> <span class="n">processor</span><span class="p">):</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">).</span><span class="nf">convert</span><span class="p">(</span><span class="sh">"</span><span class="s">RGB</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>  <span class="c1"># Resize to match input size for FLAVA
</span>    <span class="n">inputs</span> <span class="o">=</span> <span class="nf">processor</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="p">[</span><span class="n">caption</span><span class="p">],</span> <span class="n">images</span><span class="o">=</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">image</span>
</code></pre></div></div> <h2 id="applying-grad-cam-to-understand-image-regions-of-interest"><strong>Applying Grad-CAM to Understand Image Regions of Interest</strong></h2> <p>we implement Grad-CAM (Gradient-weighted Class Activation Mapping), which is commonly used to interpret CNN-based models. Grad-CAM helps us visualize which parts of the image are influencing the model’s decision the most.</p> <p><strong>Why use Grad-CAM?</strong></p> <p>Grad-CAM provides insights into the spatial regions that are most relevant for a specific task. It complements the attention map by showing low-level features captured by the image encoder. The heatmap generated by Grad-CAM will later be overlayed on the image to highlight important areas.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Function to apply Grad-CAM
</span><span class="k">def</span> <span class="nf">apply_gradcam</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
    <span class="n">pooled_grads</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="n">activations</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
    <span class="n">pooled_grads</span> <span class="o">=</span> <span class="n">pooled_grads</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">pooled_grads</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">activations</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">*=</span> <span class="n">pooled_grads</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="n">heatmap</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">heatmap</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">heatmap</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">heatmap</span> <span class="o">=</span> <span class="n">heatmap</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">heatmap</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">heatmap</span>

</code></pre></div></div> <h2 id="overlaying-the-grad-cam-heatmap-on-image"><strong>Overlaying the Grad-CAM Heatmap on Image</strong></h2> <p>We use <strong>OpenCV</strong> to overlay the heatmap and the image, allowing us to visually inspect which regions of the image are most relevant for the model’s decision.</p> <p><strong>Key considerations</strong>:</p> <p>Overlaying the heatmap allows us to interpret which parts of the image correspond to the model’s attention. This method provides a holistic view of the interaction between low-level image features and high-level semantics.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Function to overlay Grad-CAM heatmap on image
</span><span class="k">def</span> <span class="nf">overlay_heatmap_on_image</span><span class="p">(</span><span class="n">heatmap</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
    <span class="n">heatmap_resized</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span><span class="n">heatmap</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">image</span><span class="p">.</span><span class="n">size</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">heatmap_resized</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">uint8</span><span class="p">(</span><span class="mi">255</span> <span class="o">*</span> <span class="n">heatmap_resized</span><span class="p">)</span>
    <span class="n">heatmap_resized</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">applyColorMap</span><span class="p">(</span><span class="n">heatmap_resized</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLORMAP_JET</span><span class="p">)</span>
    <span class="n">image_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">superimposed_image</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">addWeighted</span><span class="p">(</span><span class="n">image_array</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="n">heatmap_resized</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">superimposed_image</span>
</code></pre></div></div> <h2 id="visualizing-attention-maps-for-image-and-text"><strong>Visualizing Attention Maps for Image and Text</strong></h2> <p>we use the attention weights captured in previous layers to generate attention maps. These maps help us visualize how the model attends to different parts of the image when processing specific text tokens.</p> <p><strong>Why use Attention Maps?</strong></p> <p><strong>Multimodal Alignment</strong>: Shows how different regions of the image are aligned with specific text tokens.</p> <p><strong>Interpretability</strong>: Helps us understand how the model processes image-text pairs, a critical step in visual grounding tasks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Function to visualize attention map for image and text
</span><span class="k">def</span> <span class="nf">visualize_attention_map</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">attention_weights</span><span class="p">,</span> <span class="n">text_input</span><span class="p">,</span> <span class="n">processor</span><span class="p">,</span> <span class="n">layer_num</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">head_num</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">processor</span><span class="p">.</span><span class="n">tokenizer</span>
    <span class="n">text_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text_input</span><span class="p">)</span>

    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
    <span class="p">])</span>
    <span class="n">image_tensor</span> <span class="o">=</span> <span class="nf">transform</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

    <span class="n">attention_map</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="p">[</span><span class="n">layer_num</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">head_num</span><span class="p">].</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
    <span class="n">attention_map</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">attention_map</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">attention_map</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">197</span><span class="p">:</span>
        <span class="n">attention_map</span> <span class="o">=</span> <span class="n">attention_map</span><span class="p">[</span><span class="mi">1</span><span class="p">:].</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">)</span>

    <span class="n">attention_map</span> <span class="o">=</span> <span class="p">(</span><span class="n">attention_map</span> <span class="o">-</span> <span class="n">attention_map</span><span class="p">.</span><span class="nf">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">attention_map</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">attention_map</span><span class="p">.</span><span class="nf">min</span><span class="p">())</span>
    <span class="n">attention_map_resized</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">interpolate</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">attention_map</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">bilinear</span><span class="sh">'</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>

    <span class="n">image_np</span> <span class="o">=</span> <span class="n">image_tensor</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">).</span><span class="nf">numpy</span><span class="p">()</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">image_np</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">attention_map_resized</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">jet</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Attention Map for: </span><span class="si">{</span><span class="n">text_input</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h2 id="get-activations-and-gradients-for-grad-cam"><strong>Get Activations and Gradients for Grad-CAM</strong></h2> <p>we define a helper function that captures the activations and gradients necessary for Grad-CAM. We attach hooks to the model’s image embeddings and compute the gradients with respect to the image features.</p> <p>This step ensures that we can access the internal representations of the model for better interpretability.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Function to get activations and gradients
</span><span class="k">def</span> <span class="nf">get_activations_and_grads</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">image_input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">'</span><span class="s">pixel_values</span><span class="sh">'</span><span class="p">]</span>

    <span class="n">activations</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">hook_activations</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="n">activations</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">hook_grads</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="n">grads</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">grad_output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">handle_activations</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">image_model</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">patch_embeddings</span><span class="p">.</span><span class="n">projection</span><span class="p">.</span><span class="nf">register_forward_hook</span><span class="p">(</span><span class="n">hook_activations</span><span class="p">)</span>
    <span class="n">handle_grads</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">image_model</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">patch_embeddings</span><span class="p">.</span><span class="n">projection</span><span class="p">.</span><span class="nf">register_backward_hook</span><span class="p">(</span><span class="n">hook_grads</span><span class="p">)</span>

    <span class="n">image_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_image_features</span><span class="p">(</span><span class="n">pixel_values</span><span class="o">=</span><span class="n">image_input</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">image_embeddings</span><span class="p">.</span><span class="nf">norm</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">handle_activations</span><span class="p">.</span><span class="nf">remove</span><span class="p">()</span>
    <span class="n">handle_grads</span><span class="p">.</span><span class="nf">remove</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">activations</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <h2 id="process-and-visualize-images"><strong>Process and Visualize Images</strong></h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">random</span>

<span class="c1"># Function to process and visualize 10 random unique images
</span><span class="k">def</span> <span class="nf">process_random_images</span><span class="p">(</span><span class="n">captions_data</span><span class="p">,</span> <span class="n">processor</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">num_images</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># Randomly sample 10 unique images
</span>    <span class="n">random_images</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">captions_data</span><span class="p">,</span> <span class="n">num_images</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">image_info</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">random_images</span><span class="p">):</span>
        <span class="n">image_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="sh">"</span><span class="s">./flickr30k/Images</span><span class="sh">"</span><span class="p">,</span> <span class="n">image_info</span><span class="p">[</span><span class="sh">'</span><span class="s">filename</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">caption</span> <span class="o">=</span> <span class="n">image_info</span><span class="p">[</span><span class="sh">'</span><span class="s">captions</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Preprocess image and caption
</span>        <span class="n">inputs</span><span class="p">,</span> <span class="n">original_image</span> <span class="o">=</span> <span class="nf">preprocess_image_caption</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">caption</span><span class="p">,</span> <span class="n">processor</span><span class="p">)</span>

        <span class="c1"># Get activations and gradients for Grad-CAM
</span>        <span class="n">activations</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="nf">get_activations_and_grads</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

        <span class="c1"># Apply Grad-CAM and visualize
</span>        <span class="n">heatmap</span> <span class="o">=</span> <span class="nf">apply_gradcam</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
        <span class="n">superimposed_image</span> <span class="o">=</span> <span class="nf">overlay_heatmap_on_image</span><span class="p">(</span><span class="n">heatmap</span><span class="p">,</span> <span class="n">original_image</span><span class="p">)</span>

        <span class="c1"># Plot original, heatmap, and superimposed image
</span>        <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">original_image</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Original Image</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">heatmap</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">jet</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Grad-CAM Heatmap</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">superimposed_image</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Grad-CAM Overlaid</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Visual Grounding for Random Image </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">caption</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

        <span class="c1"># Visualize attention map for the corresponding text and image
</span>        <span class="nf">visualize_attention_map</span><span class="p">(</span><span class="n">original_image</span><span class="p">,</span> <span class="n">attention_weights</span><span class="p">,</span> <span class="n">caption</span><span class="p">,</span> <span class="n">processor</span><span class="p">)</span>
</code></pre></div></div> <h2 id="loading-captions-and-initializing-the-flava-model"><strong>Loading Captions and Initializing the FLAVA Model</strong></h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load captions and images
</span><span class="n">caption_file</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./flickr30k/captions.txt</span><span class="sh">"</span>
<span class="n">captions_data</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">caption_file</span><span class="p">,</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">reader</span> <span class="o">=</span> <span class="n">csv</span><span class="p">.</span><span class="nf">reader</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="nf">next</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">:</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">image_filename</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">strip</span><span class="p">()</span>
        <span class="n">caption</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">strip</span><span class="p">()</span>
        <span class="n">captions_data</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">'</span><span class="s">filename</span><span class="sh">'</span><span class="p">:</span> <span class="n">image_filename</span><span class="p">,</span> <span class="sh">'</span><span class="s">captions</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="n">caption</span><span class="p">]})</span>

<span class="c1"># Initialize model and processor
</span><span class="n">processor</span> <span class="o">=</span> <span class="n">FlavaProcessor</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">facebook/flava-full</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FlavaModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">facebook/flava-full</span><span class="sh">"</span><span class="p">).</span><span class="nf">eval</span><span class="p">()</span>

<span class="c1"># Register hook for capturing attention weights
</span><span class="k">for</span> <span class="n">layer_num</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">image_model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layer</span><span class="p">):</span>
    <span class="n">layer</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="nf">register_forward_hook</span><span class="p">(</span><span class="n">get_attention_weights_hook</span><span class="p">)</span>
</code></pre></div></div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flavahuggingface.PNG-480.webp 480w,/assets/img/flavahuggingface.PNG-800.webp 800w,/assets/img/flavahuggingface.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/flavahuggingface.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h2 id="run-visualization-on-random-images"><strong>Run Visualization on Random Images</strong></h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Process and visualize random images
</span><span class="nf">process_random_images</span><span class="p">(</span><span class="n">captions_data</span><span class="p">,</span> <span class="n">processor</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">num_images</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/1g-480.webp 480w,/assets/img/1g-800.webp 800w,/assets/img/1g-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/1g.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/1a-480.webp 480w,/assets/img/1a-800.webp 800w,/assets/img/1a-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/1a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2g-480.webp 480w,/assets/img/2g-800.webp 800w,/assets/img/2g-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2g.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2a-480.webp 480w,/assets/img/2a-800.webp 800w,/assets/img/2a-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3g-480.webp 480w,/assets/img/3g-800.webp 800w,/assets/img/3g-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/3g.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3a-480.webp 480w,/assets/img/3a-800.webp 800w,/assets/img/3a-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/3a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/4g-480.webp 480w,/assets/img/4g-800.webp 800w,/assets/img/4g-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/4g.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/4a-480.webp 480w,/assets/img/4a-800.webp 800w,/assets/img/4a-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/4a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/6g-480.webp 480w,/assets/img/6g-800.webp 800w,/assets/img/6g-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/6g.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/6a-480.webp 480w,/assets/img/6a-800.webp 800w,/assets/img/6a-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/6a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7g-480.webp 480w,/assets/img/7g-800.webp 800w,/assets/img/7g-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/7g.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7a-480.webp 480w,/assets/img/7a-800.webp 800w,/assets/img/7a-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/7a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/8g-480.webp 480w,/assets/img/8g-800.webp 800w,/assets/img/8g-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/8g.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/8a-480.webp 480w,/assets/img/8a-800.webp 800w,/assets/img/8a-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/8a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9g-480.webp 480w,/assets/img/9g-800.webp 800w,/assets/img/9g-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/9g.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9a-480.webp 480w,/assets/img/9a-800.webp 800w,/assets/img/9a-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/9a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h2 id="conclusions"><strong>Conclusions</strong></h2> <p>By looking at the visual output of the Grad-CAM heatmap and the attention map applied to the image-2 of a band performing with flame-throwing effects, we can draw several technical insights:</p> <p><strong>Grad-CAM Insights</strong>:</p> <p><strong>Heatmap Activation Focus</strong>:</p> <p>The Grad-CAM heatmap highlights the areas in the image where the model’s attention is most concentrated. The color variations from blue to red indicate different levels of focus, with red being the highest activation. The overlaid heatmap shows that the model is strongly focusing on the flame-throwing areas and the center stage where the band is performing. This suggests that the model recognizes the key elements in the image—the performers and the dramatic flames—as important visual cues.</p> <p>The strong activation over the flame-throwers suggests that the model associates “<em>flames</em>” with the text “<em>flame throwing effects</em>” in the caption. The attention on the center, where the lead performer stands, suggests that the model also recognizes a human figure, which is in line with the “<em>band dressed in black</em>” part of the caption. This implies that the model is correctly aligning the visual cues (flames and performers) with the descriptive elements in the text.</p> <p><strong>Use of Grad-CAM</strong>:</p> <p>Grad-CAM works by using the gradients flowing into the final convolutional layer to understand which parts of the image most strongly impact the model’s decisions. In this case, the flame effects and the central figure are given the most attention, showing the model’s sensitivity to both human presence and the flames, which are critical aspects of the caption.</p> <p><strong>Attention Map Insights</strong>:</p> <p><strong>Text-Image Alignment</strong>:</p> <p>The attention map overlays show how different regions of the image are linked to specific tokens in the caption. Since this is a general attention map (for all tokens), it highlights regions that contribute most to understanding the caption as a whole. Areas like the flames and the stage, where the band is standing, receive more attention (yellow/red regions), while the background and parts without much relevance to the caption remain in cooler colors (blue/green).</p> <p><strong>Semantic Consistency</strong>:</p> <p>The attention map suggests a high degree of semantic consistency. The regions of the image that the model attends to (flames, performers) match with the words “band,” “black,” and “flame throwing effects” from the caption. This shows the model’s ability to correctly attend to and map visual features with descriptive textual tokens. Broader Focus:</p> <p><strong>Compared to Grad-CAM</strong>, which highlights the most critical regions, the attention map provides a broader focus, giving weight to multiple parts of the image. This is typical in multimodal models, where multiple regions contribute to the interpretation of different parts of the caption. For instance, the flames and stage structure are attended to because they align with both “flame throwing effects” and “band.”</p> <p><strong>Model’s Visual Grounding Ability:</strong></p> <p>The model demonstrates effective visual grounding, where it successfully identifies and focuses on the key elements in the image—flames and band members—that are explicitly mentioned in the caption. The Grad-CAM heatmap confirms that the model is highly sensitive to these regions during the classification process, while the attention map shows how well the model distributes its focus across the relevant visual features.</p> <p><strong>Balanced Multimodal Attention:</strong></p> <p>The overlay of the attention map suggests that the model’s attention is distributed well across the important visual features without neglecting any key areas. Both the flames and the performers receive adequate attention, reflecting the importance of those features in understanding the caption.</p> <p><strong>Further Improvements:</strong></p> <p>While the model performs well in this case, it may sometimes overlook finer details not explicitly mentioned in the caption (e.g., audience in the background), as seen by the cooler areas in the attention map. Further fine-tuning or adjusting the model to focus on a wider range of features could improve its performance on more complex images and captions.</p> <p><strong>Practical Applications:</strong></p> <p>This visualization technique can be especially useful in domains like content-based image retrieval, video captioning, and human-computer interaction. By ensuring that a model’s attention aligns well with relevant content, one can improve the accuracy and relevance of AI systems interpreting visual data.</p> <h2 id="references"><strong>References</strong></h2> <ul> <li>FLAVA Paper: <a href="https://arxiv.org/abs/2112.04482" rel="external nofollow noopener" target="_blank">FLAVA: A Foundational Language and Vision Alignment Model</a> </li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Aarti Sharma. </div> </footer> <script src="" integrity="" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="" integrity="" crossorigin="anonymous"></script> <script defer src="" integrity="" crossorigin="anonymous"></script> <script defer src="" integrity="" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="" integrity="" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="" integrity="" crossorigin="anonymous"></script> <script defer src="" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script> </script> </body> </html>