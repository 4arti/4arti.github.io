<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://4arti.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://4arti.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-18T14:12:23+00:00</updated><id>https://4arti.github.io/feed.xml</id><title type="html">blank</title><subtitle>Collections of my thoughts, work and notes. </subtitle><entry><title type="html">Exploring the Landscape of Convolutional Neural Networks</title><link href="https://4arti.github.io/blog/2024/cnn-review/" rel="alternate" type="text/html" title="Exploring the Landscape of Convolutional Neural Networks"/><published>2024-05-23T00:00:00+00:00</published><updated>2024-05-23T00:00:00+00:00</updated><id>https://4arti.github.io/blog/2024/cnn-review</id><content type="html" xml:base="https://4arti.github.io/blog/2024/cnn-review/"><![CDATA[<blockquote> <p><strong>Objective</strong> :</p> <ul> <li>Fundamentals of CNN</li> <li>Understanding the Evolution of CNN Architectures</li> <li>Practical Applications</li> </ul> </blockquote> <hr/> <h2 id="table-of-contents">Table of Contents</h2> <ol> <li><a href="#fundamentals-of-convolutional-neural-networks-cnn">Fundamentals of Convolutional Neural Networks (CNN)</a></li> <li><a href="#evolution-of-cnn-architectures">Evolution of CNN Architectures</a> <ul> <li><a href="#lenet-5-1998-the-pioneer">LeNet-5 (1998)</a></li> <li><a href="#alexnet-2012-the-breakthrough">AlexNet (2012)</a></li> <li><a href="#vggnet-2014-deeper-but-simpler">VGGNet (2014)</a></li> <li><a href="#googlenet-inception-2014-smarter-not-just-deeper">GoogLeNet (Inception, 2014)</a></li> <li><a href="#resnet-2015-deeper-networks-without-the-pain">ResNet (2015)</a></li> <li><a href="#efficientnet-2019-scaling-done-right">EfficientNet (2019)</a></li> </ul> </li> <li><a href="#practical-applications-of-cnn-architectures">Practical Applications of CNN Architectures</a> <ul> <li><a href="#31-lenet-5-1998">LeNet-5 (1998)</a></li> <li><a href="#32-alexnet-2012">AlexNet (2012)</a></li> <li><a href="#33-vggnet-2014">VGGNet (2014)</a></li> <li><a href="#34-googlenet-inception-2014">GoogLeNet (Inception, 2014)</a></li> <li><a href="#35-resnet-2015">ResNet (2015)</a></li> <li><a href="#36-efficientnet-2019">EfficientNet (2019)</a></li> </ul> </li> <li><a href="#references">References</a></li> <li><a href="#conclusion">Conclusion</a></li> </ol> <hr/> <h2 id="1-fundamentals-of-convolutional-neural-networkscnn">1. Fundamentals of Convolutional Neural Networks(CNN)</h2> <p>At their core, CNNs leverage a series of convolutional layers, pooling layers, and fully connected layers to process input data. The convolutional layers apply learnable filters to input images, extracting features such as edges, textures, and patterns. Pooling layers then downsample the feature maps, reducing their spatial dimensions while preserving important features. Finally, fully connected layers combine these features to make predictions about the input data.</p> <blockquote> <p>A key difference between <em>Dense</em> layer and a <em>Convolutional</em> layer is that, dense layer learns global patterns whereas convolutional layer learns local patterns. This key distinction allows convolutional layers to capture spatial hierarchies and learn translational invariance.</p> </blockquote> <p><strong>Translational Invariance:</strong></p> <p>A CNN can learn to detect patterns regardless of their position within the input image. For example, a CNN trained to recognize cats should be able to identify a cat regardless of whether it appears in the center or the corner of the image. Translational invariance is a desirable property in tasks such as object recognition and classification, as it allows models to generalize better to unseen data and variations in object position or orientation</p> <p><strong>Spatial Hierarchy:</strong></p> <p>At the lowest layers of a CNN, simple features such as edges, corners, and textures are learned. These features have a small spatial scope and represent basic visual elements present in the input data. As we move deeper into the network, features become more complex and encompass larger spatial regions. For example, a deeper layer might learn features like shapes, object parts, or entire objects.</p> <p>The basic components of a CNN architecture are:</p> <ul> <li>Convolutional Layer</li> <li>Pooling Layer</li> <li>Activation Function</li> <li>Batch Normalization</li> <li>Dropout</li> <li>Fully Connected Layer</li> </ul> <h3 id="convolutional-layer">Convolutional Layer</h3> <p>A convolutional layer in a Convolutional Neural Network (CNN) performs feature extraction by applying a set of learnable filters (kernels) to the input data. Each filter scans across the input data, computing the dot product between its weights and the values in its receptive field, resulting in a feature map that highlights the presence of certain patterns or features in the input.</p> <hr/> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/convolution_operation-480.webp 480w,/assets/img/convolution_operation-800.webp 800w,/assets/img/convolution_operation-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/convolution_operation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Convolution Operation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Convolution operation <a href="https://www.researchgate.net/figure/Convolution-operation_fig2_355656417">Source</a> </div> <p><br/> Key aspects of a convolutional layer: &lt;/br&gt;</p> <p><strong>Feature Detection</strong>: The convolutional filters detect various features such as edges, textures, and shapes present in the input data.</p> <p><strong>Spatial Hierarchies</strong>: Through the hierarchy of layers, convolutional layers capture increasingly complex and abstract spatial structures, building upon features learned in previous layers.</p> <p><strong>Parameter Sharing</strong>: By sharing weights across different spatial locations, convolutional layers learn translational invariance, enabling them to detect patterns regardless of their position within the input.</p> <h3 id="pooling-layer">Pooling Layer</h3> <p>Pooling layers reduce the spatial dimensions of feature maps, making the model more computationally efficient while retaining the most important information. Pooling also helps to make CNNs more robust to small translations in the input.</p> <p>The two most common types of pooling are:</p> <ul> <li> <p><strong>Max Pooling</strong>: This method takes the maximum value from a set of values in a small window of the feature map. Max pooling focuses on the most prominent features and is widely used in image-based tasks.</p> <p>Example: If the window contains values <code class="language-plaintext highlighter-rouge">[1, 3, 2, 8]</code>, max pooling will output <code class="language-plaintext highlighter-rouge">8</code>.</p> </li> <li> <p><strong>Average Pooling</strong>: It computes the average of the values in a window, which smooths out the feature maps and is often used for downscaling.</p> <p>Example: For the same window <code class="language-plaintext highlighter-rouge">[1, 3, 2, 8]</code>, average pooling outputs <code class="language-plaintext highlighter-rouge">(1+3+2+8)/4 = 3.5</code>.</p> </li> </ul> <p><strong>Why Pooling?</strong> Pooling helps reduce the dimensionality of feature maps and ensures that only the essential features are preserved. This makes the network faster and less prone to overfitting.</p> <hr/> <h3 id="activation-functions">Activation Functions</h3> <p>Activation functions introduce non-linearity into neural networks, which is critical for learning complex patterns and making accurate predictions. CNNs rely on a few key activation functions:</p> <ul> <li> <p><strong>ReLU (Rectified Linear Unit)</strong>: The most popular activation function in CNNs. It simply returns the input if it’s positive and returns zero if it’s negative.</p> <p>Example: For an input of <code class="language-plaintext highlighter-rouge">[-2, 0, 3]</code>, ReLU outputs <code class="language-plaintext highlighter-rouge">[0, 0, 3]</code>.</p> <p><strong>Why ReLU?</strong> ReLU solves the vanishing gradient problem that plagued earlier activation functions (like Sigmoid), allowing for faster and more efficient training.</p> </li> <li> <p><strong>Leaky ReLU</strong>: A variation of ReLU that allows a small gradient even for negative inputs. This avoids the problem of dead neurons (neurons that never activate).</p> </li> <li> <p><strong>Sigmoid</strong>: This function squashes the input into a range between 0 and 1, making it ideal for binary classification. However, it tends to suffer from vanishing gradients in deep networks.</p> </li> <li> <p><strong>Tanh</strong>: Similar to Sigmoid but squashes the input between -1 and 1. While it solves the zero-centered problem of Sigmoid, it still suffers from vanishing gradients.</p> </li> </ul> <hr/> <h3 id="batch-normalization">Batch Normalization</h3> <p><strong>Batch Normalization (BatchNorm)</strong> is a technique that normalizes the inputs to each layer in the network, helping to stabilize and accelerate the training process. By normalizing the activations, BatchNorm ensures that each layer receives inputs with a consistent distribution, which makes training deep networks much more efficient.</p> <p><strong>Benefits of BatchNorm</strong>:</p> <ul> <li><strong>Faster Convergence</strong>: Normalizing the inputs allows for higher learning rates, which speeds up the convergence of the model.</li> <li><strong>Reduced Sensitivity to Initialization</strong>: Models are less sensitive to the initial weight values, making the training process smoother.</li> <li><strong>Regularization</strong>: It provides a slight regularization effect, similar to dropout, which helps prevent overfitting.</li> </ul> <hr/> <h3 id="dropout">Dropout</h3> <p><strong>Dropout</strong> is a regularization technique where, during training, a random subset of neurons in a layer is “dropped” (i.e., set to zero). This forces the network to learn more robust features and prevents over-reliance on specific neurons, which helps reduce overfitting.</p> <p><strong>How Dropout Works</strong>:</p> <ul> <li>During each training step, a fraction (e.g., 50%) of neurons is dropped randomly.</li> <li>During inference (when the model is making predictions), all neurons are used, but their activations are scaled down by the dropout rate to balance the network.</li> </ul> <p><strong>Why It Works</strong>: Dropout essentially simulates training many different networks simultaneously, which helps in learning generalized features.</p> <hr/> <h3 id="fully-connected-layer">Fully Connected Layer</h3> <p>The <strong>Fully Connected Layer</strong> (or Dense Layer) comes at the end of the CNN architecture, where all neurons are connected to every neuron in the previous layer. This is where the final decision-making happens. After the convolutional and pooling layers extract the spatial features, the fully connected layers use these features to predict the final class (in the case of classification tasks) or output value (in regression tasks).</p> <p><strong>Why Fully Connected Layers?</strong> While convolutional layers capture local features, the fully connected layer combines these features globally and applies them to the classification or regression problem.</p> <hr/> <h2 id="2-evolution-of-cnn-architectures-from-lenet-to-efficientnet">2. Evolution of CNN Architectures: From LeNet to EfficientNet</h2> <p>The architecture of CNNs has evolved significantly over the years. Here’s a deeper look into how each architecture addressed the challenges faced by its predecessor:</p> <h3 id="21-lenet-5-1998-the-pioneer">2.1. LeNet-5 (1998): The Pioneer</h3> <p>LeNet-5, designed by Yann LeCun, is one of the first CNNs that made practical applications like digit recognition on the MNIST dataset possible. This architecture had two convolutional layers followed by two fully connected layers. Despite its simplicity, it achieved remarkable results for its time.</p> <p><strong>Flaws</strong>:</p> <ul> <li><strong>Shallow Architecture</strong>: LeNet-5 has very few layers, which limited its ability to capture complex patterns in larger datasets.</li> <li><strong>Overfitting</strong>: Small networks like LeNet-5 were prone to overfitting, especially when applied to more complex tasks with larger datasets.</li> </ul> <p><strong>What Came Next</strong>: As datasets grew, deeper architectures were needed to capture more abstract features and avoid overfitting.</p> <hr/> <h3 id="22-alexnet-2012-the-breakthrough">2.2. AlexNet (2012): The Breakthrough</h3> <p>AlexNet, created by Alex Krizhevsky, made CNNs a household name after it won the 2012 ImageNet competition by a large margin. AlexNet was deeper and wider than LeNet-5, using five convolutional layers followed by three fully connected layers. The innovation came from using ReLU activations, dropout for regularization, and GPU acceleration for training large datasets.</p> <p><strong>Flaws</strong>:</p> <ul> <li><strong>High Computational Cost</strong>: AlexNet introduced a deeper architecture, but it came at the cost of high computational requirements.</li> <li><strong>Manual Design</strong>: There was a lot of manual tuning involved, such as deciding the number of layers, neurons, and filters.</li> </ul> <p><strong>How It Was Resolved</strong>: The rise of even deeper architectures called for strategies to reduce computation and make models more efficient.</p> <hr/> <h3 id="23-vggnet-2014-deeper-but-simpler">2.3. VGGNet (2014): Deeper but Simpler</h3> <p>VGGNet, proposed by the Visual Geometry Group at Oxford, took CNN depth to the next level by stacking small 3x3 convolutional filters sequentially. The most popular models, VGG16 and VGG19, used 16 and 19 layers, respectively. By using smaller filters, VGGNet made the network simpler and more uniform, focusing on depth.</p> <p><strong>Flaws</strong>:</p> <ul> <li><strong>Too Many Parameters</strong>: Despite the simplicity of the design, VGGNet has a huge number of parameters (138 million in VGG16!), making it resource-intensive.</li> <li><strong>Slow Training</strong>: The large number of parameters slowed down training, requiring a lot of computational power and memory.</li> </ul> <p><strong>What Came Next</strong>: The need for efficient architectures that can achieve similar performance but with fewer parameters led to GoogLeNet and ResNet.</p> <hr/> <h3 id="24-googlenet-inception-2014-smarter-not-just-deeper">2.4. GoogLeNet (Inception, 2014): Smarter, Not Just Deeper</h3> <p>GoogLeNet introduced the <strong>Inception module</strong>, a game-changing concept where different types of convolutions (1x1, 3x3, and 5x5) were applied in parallel. This allowed the network to capture patterns at multiple scales, increasing both efficiency and accuracy. GoogLeNet used fewer parameters than VGGNet, thanks to global average pooling instead of fully connected layers.</p> <p><strong>Flaws</strong>:</p> <ul> <li><strong>Complexity in Design</strong>: Although the Inception module was highly efficient, its design was more complex and not as intuitive as previous architectures.</li> <li><strong>Manual Crafting of Inception Modules</strong>: Inception modules needed to be manually designed, which required a lot of expertise and fine-tuning.</li> </ul> <p><strong>How It Was Resolved</strong>: Residual connections in ResNet simplified the training of very deep networks, reducing the risk of manual design flaws.</p> <hr/> <h3 id="25-resnet-2015-deeper-networks-without-the-pain">2.5. ResNet (2015): Deeper Networks Without the Pain</h3> <p>ResNet solved the problem of “vanishing gradients” that plagued earlier deep networks. It introduced <strong>residual connections</strong>, or “skip connections,” allowing gradients to flow more easily through the network during backpropagation. This breakthrough made it possible to train networks with over 100 layers.</p> <p><strong>Flaws</strong>:</p> <ul> <li><strong>Computational Expense</strong>: Despite solving the depth problem, ResNet is still computationally expensive.</li> <li><strong>Redundant Layers</strong>: Not every layer in a deep ResNet necessarily contributes equally to learning, which can lead to redundancy.</li> </ul> <p><strong>What Came Next</strong>: While ResNet allowed for extreme depth, there was still a demand for architectures that could scale both depth and width efficiently—leading to EfficientNet.</p> <hr/> <h3 id="26-efficientnet-2019-scaling-done-right">2.6. EfficientNet (2019): Scaling Done Right</h3> <p>EfficientNet revolutionized the way we think about scaling architectures. Instead of increasing depth, width, or input resolution arbitrarily, EfficientNet used a <strong>compound scaling method</strong> to scale all dimensions uniformly. This balanced approach led to state-of-the-art performance while using fewer resources than previous models.</p> <p><strong>Flaws</strong>:</p> <ul> <li><strong>Complex to Implement</strong>: EfficientNet requires careful compound scaling, which adds complexity in its design.</li> <li><strong>Tuned for Specific Tasks</strong>: While highly efficient, EfficientNet might not always generalize well to tasks outside of image classification without modifications.</li> </ul> <p><strong>What It Resolved</strong>: EfficientNet provided a balance of depth, width, and resolution, making it the most efficient CNN architecture in terms of accuracy and computational cost.</p> <hr/> <h2 id="3-practical-applications-of-cnn-architectures">3. Practical Applications of CNN Architectures</h2> <p>Convolutional Neural Networks (CNNs) have been instrumental in solving real-world problems across various industries. Below, we explore the practical applications of the CNN architectures discussed earlier, with links to papers, websites, and working demos where possible.</p> <h3 id="31-lenet-5-1998">3.1. LeNet-5 (1998)</h3> <p><strong>Application</strong>: Handwritten digit recognition was the primary application of LeNet-5, which worked excellently on datasets like MNIST. It laid the groundwork for document digitization, postal address reading, and similar tasks.</p> <ul> <li><strong>Paper</strong>: <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">LeNet-5: Gradient-Based Learning Applied to Document Recognition</a></li> <li><strong>Demo</strong>: <a href="https://github.com/sujaybabruwad/LeNet-5-Implementation">MNIST Handwritten Digit Recognition using LeNet-5</a></li> </ul> <p><strong>Use Case</strong>: Used in automated banking systems for reading checks and digitizing documents.</p> <hr/> <h3 id="32-alexnet-2012">3.2. AlexNet (2012)</h3> <p><strong>Application</strong>: AlexNet’s primary claim to fame was winning the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC). It revolutionized image classification and object detection. AlexNet has been widely used in applications such as object detection in autonomous vehicles, facial recognition, and medical image classification.</p> <ul> <li><strong>Paper</strong>: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)</a></li> <li><strong>Demo</strong>: <a href="https://pytorch.org/vision/stable/models/generated/torchvision.models.alexnet.html">AlexNet in PyTorch</a></li> </ul> <p><strong>Use Case</strong>: Powering computer vision applications like object detection in self-driving cars and facial recognition systems.</p> <hr/> <h3 id="33-vggnet-2014">3.3. VGGNet (2014)</h3> <p><strong>Application</strong>: VGGNet’s deep structure and smaller convolutional filters made it popular in image classification, especially in fine-grained tasks like facial recognition and medical imaging. VGG16 and VGG19 models are used in applications that require high accuracy.</p> <ul> <li><strong>Paper</strong>: <a href="https://arxiv.org/pdf/1409.1556.pdf">Very Deep Convolutional Networks for Large-Scale Image Recognition (VGGNet)</a></li> <li><strong>Demo</strong>: <a href="https://keras.io/api/applications/vgg/">VGGNet for Image Classification</a></li> </ul> <p><strong>Use Case</strong>: Used for fine-grained image classification tasks, facial recognition, and deep learning-based anomaly detection in medical imaging.</p> <hr/> <h3 id="34-googlenet-inception-2014">3.4. GoogLeNet (Inception, 2014)</h3> <p><strong>Application</strong>: GoogLeNet’s Inception module allows for efficient image classification, which has been used in applications such as video analysis, security surveillance, and large-scale object recognition systems. Its low parameter count makes it suitable for mobile devices and embedded systems.</p> <ul> <li><strong>Paper</strong>: <a href="https://arxiv.org/pdf/1409.4842.pdf">Going Deeper with Convolutions (GoogLeNet)</a></li> <li><strong>Demo</strong>: <a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/inception">GoogLeNet with TensorFlow</a></li> </ul> <p><strong>Use Case</strong>: Used in video recognition systems, real-time surveillance systems, and for large-scale object recognition tasks.</p> <hr/> <h3 id="35-resnet-2015">3.5. ResNet (2015)</h3> <p><strong>Application</strong>: ResNet is widely used in image classification, object detection, and semantic segmentation. Its use of residual connections allows for very deep networks, which makes it applicable in medical image analysis, autonomous vehicles, and high-end computer vision tasks.</p> <ul> <li><strong>Paper</strong>: <a href="https://arxiv.org/pdf/1512.03385.pdf">Deep Residual Learning for Image Recognition (ResNet)</a></li> <li><strong>Demo</strong>: <a href="https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet50.html">ResNet50 in PyTorch</a></li> </ul> <p><strong>Use Case</strong>: Used in advanced applications like medical diagnostics (e.g., detecting diseases from MRI scans), drone vision, and complex industrial inspection tasks.</p> <hr/> <h3 id="36-efficientnet-2019">3.6. EfficientNet (2019)</h3> <p><strong>Application</strong>: EfficientNet has become the go-to architecture for real-time image processing tasks due to its balanced scaling of depth, width, and resolution. It is used in applications like mobile vision, automated agricultural systems (crop health monitoring), and in low-power, high-accuracy image recognition tasks.</p> <ul> <li><strong>Paper</strong>: <a href="https://arxiv.org/pdf/1905.11946.pdf">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></li> <li><strong>Demo</strong>: <a href="https://keras.io/api/applications/efficientnet/">EfficientNet in Keras</a></li> </ul> <p><strong>Use Case</strong>: Deployed in real-time image recognition on mobile devices, drones, and healthcare diagnostics with limited computational resources.</p> <h2 id="4-conclusion">4. Conclusion</h2> <p>From simple digit recognition to powering sophisticated systems in healthcare, transportation, and entertainment, CNNs have proven to be remarkably versatile. The architectures discussed above, including LeNet-5, AlexNet, VGGNet, GoogLeNet, ResNet, and EfficientNet, have opened up endless possibilities for applying deep learning in practical, real-world scenarios. Whether you are working on a mobile application or deploying a large-scale object detection system, there’s a CNN architecture tailored for your needs.</p> <h2 id="5-references">5. References</h2> <ul> <li> <p>LeNet-5: Yann LeCun, et al. “Gradient-Based Learning Applied to Document Recognition.” Proceedings of the IEEE, 1998. <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">LeNet Paper</a></p> </li> <li> <p>AlexNet: Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. “ImageNet Classification with Deep Convolutional Neural Networks.” NIPS, 2012. <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet Paper</a></p> </li> <li> <p>VGGNet: Karen Simonyan and Andrew Zisserman. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” arXiv, 2014. <a href="https://arxiv.org/pdf/1409.1556.pdf">VGGNet Paper</a></p> </li> <li> <p>GoogLeNet: Christian Szegedy, et al. “Going Deeper with Convolutions.” arXiv, 2014. <a href="https://arxiv.org/pdf/1409.4842.pdf">GoogLeNet Paper</a></p> </li> <li> <p>ResNet: Kaiming He, et al. “Deep Residual Learning for Image Recognition.” CVPR, 2015. <a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet Paper</a></p> </li> <li> <p>EfficientNet: Mingxing Tan and Quoc V. Le. “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.” ICML, 2019. <a href="https://arxiv.org/pdf/1905.11946.pdf">EfficientNet Paper</a></p> </li> </ul> <hr/>]]></content><author><name>Aarti</name></author><category term="Deep Learning"/><category term="Research"/><category term="CNN"/><summary type="html"><![CDATA[A review of CNN architectures]]></summary></entry><entry><title type="html">Paper Summary: GIT - A Generative Image-to-text Transformer for Vision and Language</title><link href="https://4arti.github.io/blog/2024/GIT/" rel="alternate" type="text/html" title="Paper Summary: GIT - A Generative Image-to-text Transformer for Vision and Language"/><published>2024-05-19T00:00:00+00:00</published><updated>2024-05-19T00:00:00+00:00</updated><id>https://4arti.github.io/blog/2024/GIT</id><content type="html" xml:base="https://4arti.github.io/blog/2024/GIT/"><![CDATA[<blockquote> <p><strong>Objective</strong> : Providing a summary of <a href="https://arxiv.org/pdf/2205.14100">GIT: A Generative Image-to-text Transformer for Vision and Language.</a> <em>Authors</em>: Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang. Microsoft Cloud and AI</p> </blockquote> <p>The format is inspired by: <a href="https://www.cs.cmu.edu/~15712/summaries.html">Paper Summaries</a></p> <hr/> <h3 id="1-three-important-things">1. Three Important Things</h3> <p><strong>A. Network Architecture</strong></p> <ul> <li>The GIT network architecture consists of an image encoder and a text decoder.</li> <li>The image encoder is based on a contrastive pre-trained model, chosen for its superior performance in recent studies.</li> <li>It takes raw images as input and generates a compact 2D feature map, which is then flattened into a list of features.</li> <li>Additional linear and layernorm layers project these features into D dimensions, serving as input for the text decoder.</li> <li>The approach involves sequential task separation: first, pre-training the image encoder with contrastive tasks, followed by joint pre-training of both the image encoder and text decoder with generation tasks.</li> <li>The text decoder utilizes a transformer module with multiple transformer blocks containing self-attention and feed-forward layers.</li> <li>Text is tokenized, embedded, and augmented with positional encoding and layernorm layers.</li> <li>Image features are concatenated with text embeddings for input to the transformer module.</li> <li>Text generation starts with a [BOS] token and continues in an auto-regressive manner until the [EOS] token or reaching maximum steps.</li> <li>A seq2seq attention mask is applied, enabling text tokens to depend on preceding tokens and all image tokens, while image tokens can attend to each other.</li> <li>This approach contrasts with unidirectional attention masks, where not every image token can rely on all others.</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/image-1-480.webp 480w,/assets/img/image-1-800.webp 800w,/assets/img/image-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/image-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Architecture of GIT" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Architecture of GIT </div> <p><strong>B. Pre-Training</strong></p> <p>Lnaguage modelling loss is used to train the model. Language modeling loss is a measure used in natural language processing tasks to assess the performance of a language model. It quantifies how well a language model predicts the next word or token in a sequence of text.</p> <p>In the context of neural networks, language modeling loss is typically computed using cross-entropy loss. Given a sequence of input tokens ​and the corresponding target tokens, the language modeling loss is calculated as the average cross-entropy loss across all tokens.</p> <p>LM (Language Modeling) is chosen over MLM (Masked Language Modeling) due to its efficiency and effectiveness, particularly for large-scale pre-training data. MLM involves masking a certain percentage of input tokens and predicting them in each iteration. However, to predict all tokens, multiple epochs are required, as the model iterates through the data several times to predict all masked tokens.</p> <p>In contrast, LM can predict all tokens in each iteration, making it more efficient for large-scale pre-training data. This efficiency is particularly beneficial when computational resources are limited, as fewer epochs are needed to train the model.</p> <p><strong>C. Fine-tuning</strong></p> <p><br/></p> <h5 id="approach-for-image-captioning-task">Approach for image captioning task:</h5> <p>As the training data format is the same as that in pre-training, the same LM task is used to fine-tune GIT.</p> <h5 id="approach-for-visual-question-answering-vqa">Approach for Visual Question Answering (VQA):</h5> <p>During fine-tuning, the model concatenates the question and the ground-truth answer into a special caption. Language modeling (LM) loss is applied only on the answer and the end-of-sequence ([EOS]) tokens. During inference, the question serves as the caption prefix, and the model predicts the completed part as the answer.</p> <h5 id="generative-model">Generative Model:</h5> <p>The model is generative without pre-defining candidate answers, even during inference. Challenges are imposed on the model as it has to predict at least two correct tokens: one for the answer and another for [EOS].</p> <h5 id="scene-text-related-vqa">Scene-Text Related VQA:</h5> <p>Existing approaches for scene-text related VQA tasks often leverage Optical Character Recognition (OCR) engines to generate key points or text information. The model does not rely on Optical Character Recognition (OCR) engines or dynamic pointer networks. Empirical evidence shows that the model learns to read scene text with large-scale pre-training. Achieves new state-of-the-art (SoTA) performance on scene text-related tasks without OCR.</p> <h5 id="adaptation-to-video-domain">Adaptation to Video Domain:</h5> <p>Although not specifically designed for videos, the model achieves competitive or new SoTA performance with a simple architecture change. Multiple frames from each video clip are sampled and independently encoded using the image encoder. Learnable temporal embeddings are added and concatenated with features from sampled frames. Final representation is utilized similarly to image representation for captioning and question answering.</p> <h5 id="application-to-image-classification">Application to Image Classification:</h5> <p>The model’s generation model is applied to image classification tasks. Class names are interpreted as image captions, and the model is fine-tuned to predict results in an auto-regressive manner. Unlike existing methods, which pre-define vocabulary and use linear layers for prediction, this approach is generation-based. Beneficial for scenarios where new data and categories are introduced, allowing continuous training without adding new parameters.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/image-2-480.webp 480w,/assets/img/image-2-800.webp 800w,/assets/img/image-2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/image-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="caption" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Captioning results of COCO-fine-tuned GIT on random samples whose prediction contains novel terms from the nocaps validation set. Novel terms, which are not in COCO training captions, are underlined. </div> <h3 id="2-limitation">2. Limitation</h3> <p>The model focuses on a pre training-and-fine tuning strategy to enhance absolute performance. However, there is uncertainty regarding how to precisely control the generated captions. Additionally, the method for performing in-context learning without updating parameters is not clearly defined. These challenges are acknowledged as areas for future research and development.</p> <h3 id="3-conclusion">3. Conclusion</h3> <p>GIT, a generative model designed for mapping images to text descriptions within a large-scale dataset of image-text pairs. GIT achieves state-of-the-art performance in tasks like image and video captioning, as well as question answering, surpassing existing benchmarks. Notably, GIT outperforms human performance on the TextCaps dataset, marking a significant achievement in natural language understanding. A unique aspect of GIT is its approach to image classification, where it predicts label names directly, diverging from traditional methods relying on fixed vocabularies. This strategy proves advantageous, particularly in handling new category data, offering flexibility and adaptability in model predictions.</p>]]></content><author><name>Aarti</name></author><category term="Deep Learning"/><category term="Research"/><category term="summaries"/><summary type="html"><![CDATA[Transformer based generative vision-language model]]></summary></entry></feed>