<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://sharma-aarti.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sharma-aarti.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-30T17:38:20+00:00</updated><id>https://sharma-aarti.github.io/feed.xml</id><title type="html">blank</title><subtitle>Collections of my thoughts, work and notes. </subtitle><entry><title type="html">Exploring the Landscape of Convolutional Neural Networks</title><link href="https://sharma-aarti.github.io/blog/2024/cnn-review/" rel="alternate" type="text/html" title="Exploring the Landscape of Convolutional Neural Networks"/><published>2024-05-23T00:00:00+00:00</published><updated>2024-05-23T00:00:00+00:00</updated><id>https://sharma-aarti.github.io/blog/2024/cnn-review</id><content type="html" xml:base="https://sharma-aarti.github.io/blog/2024/cnn-review/"><![CDATA[<blockquote> <p><strong>Objective</strong> :</p> <ul> <li>Fundamentals of CNN</li> <li>Understanding the Evolution of CNN Architectures</li> <li>Practical Applications</li> </ul> </blockquote> <hr/> <h2 id="1-fundamentals-of-convolutional-neural-networkscnn">1. Fundamentals of Convolutional Neural Networks(CNN)</h2> <p>At their core, CNNs leverage a series of convolutional layers, pooling layers, and fully connected layers to process input data. The convolutional layers apply learnable filters to input images, extracting features such as edges, textures, and patterns. Pooling layers then downsample the feature maps, reducing their spatial dimensions while preserving important features. Finally, fully connected layers combine these features to make predictions about the input data.</p> <blockquote> <p>A key difference between <em>Dense</em> layer and a <em>Convolutional</em> layer is that, dense layer learns global patterns whereas convolutional layer learns local patterns. This key distinction allows convolutional layers to capture spatial hierarchies and learn translational invariance.</p> </blockquote> <p><strong>Translational Invariance:</strong></p> <p>A CNN can learn to detect patterns regardless of their position within the input image. For example, a CNN trained to recognize cats should be able to identify a cat regardless of whether it appears in the center or the corner of the image. Translational invariance is a desirable property in tasks such as object recognition and classification, as it allows models to generalize better to unseen data and variations in object position or orientation</p> <p><strong>Spatial Hierarchy:</strong></p> <p>At the lowest layers of a CNN, simple features such as edges, corners, and textures are learned. These features have a small spatial scope and represent basic visual elements present in the input data. As we move deeper into the network, features become more complex and encompass larger spatial regions. For example, a deeper layer might learn features like shapes, object parts, or entire objects.</p> <p>The basic components of a CNN architecture are:</p> <ul> <li>Convolutional Layer</li> <li>Pooling Layer</li> <li>Activation Function</li> <li>Batch Normalization</li> <li>Dropout</li> <li>Fully Connected Layer</li> </ul> <h3 id="convolutional-layer">Convolutional Layer</h3> <p>A convolutional layer in a Convolutional Neural Network (CNN) performs feature extraction by applying a set of learnable filters (kernels) to the input data. Each filter scans across the input data, computing the dot product between its weights and the values in its receptive field, resulting in a feature map that highlights the presence of certain patterns or features in the input.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/convolution_operation-480.webp 480w,/assets/img/convolution_operation-800.webp 800w,/assets/img/convolution_operation-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/convolution_operation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Convolution Operation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Convolution operation <a href="https://www.researchgate.net/figure/Convolution-operation_fig2_355656417">Source</a> </div> <p><br/> Key aspects of a convolutional layer:</p> <p><br/></p> <p><strong>Feature Detection</strong>: The convolutional filters detect various features such as edges, textures, and shapes present in the input data.</p> <p><strong>Spatial Hierarchies</strong>: Through the hierarchy of layers, convolutional layers capture increasingly complex and abstract spatial structures, building upon features learned in previous layers.</p> <p><strong>Parameter Sharing</strong>: By sharing weights across different spatial locations, convolutional layers learn translational invariance, enabling them to detect patterns regardless of their position within the input.</p>]]></content><author><name>Aarti</name></author><category term="Deep Learning"/><category term="Research"/><category term="CNN"/><summary type="html"><![CDATA[A review of CNN architectures]]></summary></entry><entry><title type="html">Paper Summary: GIT - A Generative Image-to-text Transformer for Vision and Language</title><link href="https://sharma-aarti.github.io/blog/2024/GIT/" rel="alternate" type="text/html" title="Paper Summary: GIT - A Generative Image-to-text Transformer for Vision and Language"/><published>2024-05-19T00:00:00+00:00</published><updated>2024-05-19T00:00:00+00:00</updated><id>https://sharma-aarti.github.io/blog/2024/GIT</id><content type="html" xml:base="https://sharma-aarti.github.io/blog/2024/GIT/"><![CDATA[<blockquote> <p><strong>Objective</strong> : Providing a summary of <a href="https://arxiv.org/pdf/2205.14100">GIT: A Generative Image-to-text Transformer for Vision and Language.</a> <em>Authors</em>: Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang. Microsoft Cloud and AI</p> </blockquote> <p>The format is inspired by: <a href="https://www.cs.cmu.edu/~15712/summaries.html">Paper Summaries</a></p> <hr/> <h3 id="1-three-important-things">1. Three Important Things</h3> <p><strong>A. Network Architecture</strong></p> <ul> <li>The GIT network architecture consists of an image encoder and a text decoder.</li> <li>The image encoder is based on a contrastive pre-trained model, chosen for its superior performance in recent studies.</li> <li>It takes raw images as input and generates a compact 2D feature map, which is then flattened into a list of features.</li> <li>Additional linear and layernorm layers project these features into D dimensions, serving as input for the text decoder.</li> <li>The approach involves sequential task separation: first, pre-training the image encoder with contrastive tasks, followed by joint pre-training of both the image encoder and text decoder with generation tasks.</li> <li>The text decoder utilizes a transformer module with multiple transformer blocks containing self-attention and feed-forward layers.</li> <li>Text is tokenized, embedded, and augmented with positional encoding and layernorm layers.</li> <li>Image features are concatenated with text embeddings for input to the transformer module.</li> <li>Text generation starts with a [BOS] token and continues in an auto-regressive manner until the [EOS] token or reaching maximum steps.</li> <li>A seq2seq attention mask is applied, enabling text tokens to depend on preceding tokens and all image tokens, while image tokens can attend to each other.</li> <li>This approach contrasts with unidirectional attention masks, where not every image token can rely on all others.</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/image-1-480.webp 480w,/assets/img/image-1-800.webp 800w,/assets/img/image-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/image-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Architecture of GIT" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Architecture of GIT </div> <p><strong>B. Pre-Training</strong></p> <p>Lnaguage modelling loss is used to train the model. Language modeling loss is a measure used in natural language processing tasks to assess the performance of a language model. It quantifies how well a language model predicts the next word or token in a sequence of text.</p> <p>In the context of neural networks, language modeling loss is typically computed using cross-entropy loss. Given a sequence of input tokens ​and the corresponding target tokens, the language modeling loss is calculated as the average cross-entropy loss across all tokens.</p> <p>LM (Language Modeling) is chosen over MLM (Masked Language Modeling) due to its efficiency and effectiveness, particularly for large-scale pre-training data. MLM involves masking a certain percentage of input tokens and predicting them in each iteration. However, to predict all tokens, multiple epochs are required, as the model iterates through the data several times to predict all masked tokens.</p> <p>In contrast, LM can predict all tokens in each iteration, making it more efficient for large-scale pre-training data. This efficiency is particularly beneficial when computational resources are limited, as fewer epochs are needed to train the model.</p> <p><strong>C. Fine-tuning</strong></p> <p><br/></p> <h5 id="approach-for-image-captioning-task">Approach for image captioning task:</h5> <p>As the training data format is the same as that in pre-training, the same LM task is used to fine-tune GIT.</p> <h5 id="approach-for-visual-question-answering-vqa">Approach for Visual Question Answering (VQA):</h5> <p>During fine-tuning, the model concatenates the question and the ground-truth answer into a special caption. Language modeling (LM) loss is applied only on the answer and the end-of-sequence ([EOS]) tokens. During inference, the question serves as the caption prefix, and the model predicts the completed part as the answer.</p> <h5 id="generative-model">Generative Model:</h5> <p>The model is generative without pre-defining candidate answers, even during inference. Challenges are imposed on the model as it has to predict at least two correct tokens: one for the answer and another for [EOS].</p> <h5 id="scene-text-related-vqa">Scene-Text Related VQA:</h5> <p>Existing approaches for scene-text related VQA tasks often leverage Optical Character Recognition (OCR) engines to generate key points or text information. The model does not rely on Optical Character Recognition (OCR) engines or dynamic pointer networks. Empirical evidence shows that the model learns to read scene text with large-scale pre-training. Achieves new state-of-the-art (SoTA) performance on scene text-related tasks without OCR.</p> <h5 id="adaptation-to-video-domain">Adaptation to Video Domain:</h5> <p>Although not specifically designed for videos, the model achieves competitive or new SoTA performance with a simple architecture change. Multiple frames from each video clip are sampled and independently encoded using the image encoder. Learnable temporal embeddings are added and concatenated with features from sampled frames. Final representation is utilized similarly to image representation for captioning and question answering.</p> <h5 id="application-to-image-classification">Application to Image Classification:</h5> <p>The model’s generation model is applied to image classification tasks. Class names are interpreted as image captions, and the model is fine-tuned to predict results in an auto-regressive manner. Unlike existing methods, which pre-define vocabulary and use linear layers for prediction, this approach is generation-based. Beneficial for scenarios where new data and categories are introduced, allowing continuous training without adding new parameters.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/image-2-480.webp 480w,/assets/img/image-2-800.webp 800w,/assets/img/image-2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/image-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="caption" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Captioning results of COCO-fine-tuned GIT on random samples whose prediction contains novel terms from the nocaps validation set. Novel terms, which are not in COCO training captions, are underlined. </div> <h3 id="2-limitation">2. Limitation</h3> <p>The model focuses on a pre training-and-fine tuning strategy to enhance absolute performance. However, there is uncertainty regarding how to precisely control the generated captions. Additionally, the method for performing in-context learning without updating parameters is not clearly defined. These challenges are acknowledged as areas for future research and development.</p> <h3 id="3-conclusion">3. Conclusion</h3> <p>GIT, a generative model designed for mapping images to text descriptions within a large-scale dataset of image-text pairs. GIT achieves state-of-the-art performance in tasks like image and video captioning, as well as question answering, surpassing existing benchmarks. Notably, GIT outperforms human performance on the TextCaps dataset, marking a significant achievement in natural language understanding. A unique aspect of GIT is its approach to image classification, where it predicts label names directly, diverging from traditional methods relying on fixed vocabularies. This strategy proves advantageous, particularly in handling new category data, offering flexibility and adaptability in model predictions.</p>]]></content><author><name>Aarti</name></author><category term="Deep Learning"/><category term="Research"/><category term="summaries"/><summary type="html"><![CDATA[Transformer based generative vision-language model]]></summary></entry></feed>